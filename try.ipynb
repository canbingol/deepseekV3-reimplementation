{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from config import ModelConfig\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "dim = 64\n",
    "n_head = 16\n",
    "head_dim = dim // n_head\n",
    "base = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q = torch.rand(batch_size,n_head,seq_len,head_dim)\n",
    "k = torch.rand(batch_size,n_head,seq_len,head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 4, 2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.float().view(*q.shape[:-1], -1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 4, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6144"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*12*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Optional, Literal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    \"\"\"\n",
    "Model argümanlarını ve hiperparametreleri tanımlayan veri sınıfı.\n",
    "\n",
    "Öznitelikler (Attributes):\n",
    "    max_batch_size (int): Maksimum batch (yığın) boyutu.\n",
    "    max_seq_len (int): Maksimum dizi (sequence) uzunluğu.\n",
    "    dtype (Literal[\"bf16\", \"fp8\"]): Hesaplamalar için kullanılacak veri tipi.\n",
    "    vocab_size (int): Kelime dağarcığı (vocabulary) boyutu.\n",
    "    dim (int): Modelin genel gizli katman boyutu (embedding + hidden dim).\n",
    "    inter_dim (int): MLP (besleyici ağ) katmanları için ara katman boyutu.\n",
    "    moe_inter_dim (int): MoE (Mixture of Experts) katmanları için ara katman boyutu.\n",
    "    n_layers (int): Transformer katmanı sayısı.\n",
    "    n_dense_layers (int): Modeldeki yoğun (dense) katman sayısı.\n",
    "    n_heads (int): Dikkat (attention) başlığı sayısı.\n",
    "    n_routed_experts (int): MoE içinde yönlendirilen uzman sayısı.\n",
    "    n_shared_experts (int): MoE içinde paylaşılan (her gruba açık) uzman sayısı.\n",
    "    n_activated_experts (int): Her örnek için aktif edilen uzman sayısı.\n",
    "    n_expert_groups (int): Uzman grubu sayısı (MoE routing grupları).\n",
    "    n_limited_groups (int): MoE yönlendirmesinde sınırlandırılmış grup sayısı.\n",
    "    score_func (Literal[\"softmax\", \"sigmoid\"]): MoE yönlendirme puanlama fonksiyonu.\n",
    "    route_scale (float): Routing skorları için çarpan ölçekleme katsayısı.\n",
    "    q_lora_rank (int): Query (sorgu) projeksiyonları için LoRA rank’ı.\n",
    "    kv_lora_rank (int): Key-Value (anahtar-değer) projeksiyonları için LoRA rank’ı.\n",
    "    qk_nope_head_dim (int): Konumsal bilgi olmadan QK projeksiyonları için başlık boyutu.\n",
    "    qk_rope_head_dim (int): Rotary Positional Embedding kullanılan QK projeksiyon başlık boyutu.\n",
    "    v_head_dim (int): Value (değer) projeksiyon başlık boyutu.\n",
    "    original_seq_len (int): Modelin önceden eğitim aldığı maksimum dizgi uzunluğu.\n",
    "    rope_theta (float): Rotary positional encoding için temel (üstel frekans) değeri.\n",
    "    rope_factor (float): Rotary frekans düzeltmesi için ölçekleme katsayısı.\n",
    "    beta_fast (int): Düşük rotasyon eşiği (erken düzeltme için).\n",
    "    beta_slow (int): Yüksek rotasyon eşiği (tam düzeltme için).\n",
    "    mscale (float): Uzatılmış dikkat (extended attention) için ölçekleme katsayısı.\n",
    "    \"\"\"\n",
    "    max_batch_size: int = 8\n",
    "    max_seq_len: int = 256\n",
    "    dtype: Literal[\"bf16\", \"fp8\"] = \"bf16\"\n",
    "    vocab_size: int = 50256\n",
    "    dim: int = 1024\n",
    "    inter_dim: int = 4 * dim\n",
    "    moe_inter_dim: int = 704\n",
    "    n_layers: int = 6\n",
    "    n_dense_layers: int = 1\n",
    "    n_heads: int = 8\n",
    "    # moe\n",
    "    n_routed_experts: int = 8\n",
    "    n_shared_experts: int = 2\n",
    "    n_activated_experts: int = 4\n",
    "    n_expert_groups: int = 1\n",
    "    n_limited_groups: int = 1\n",
    "    score_func: Literal[\"softmax\", \"sigmoid\"] = \"softmax\"\n",
    "    route_scale: float = 1.\n",
    "    # mla\n",
    "    q_lora_rank: int = 0\n",
    "    kv_lora_rank: int = 256\n",
    "    qk_nope_head_dim: int = 64\n",
    "    qk_rope_head_dim: int = 32\n",
    "    v_head_dim: int = 64\n",
    "    # yarn\n",
    "    original_seq_len: int = 512\n",
    "    rope_theta: float = 10000.0\n",
    "    rope_factor: float = 40\n",
    "    beta_fast: int = 32\n",
    "    beta_slow: int = 1\n",
    "    mscale: float = 1.\n",
    "\n",
    "    # data preparing\n",
    "    shuffle: bool = True\n",
    "    drop_last: bool = True\n",
    "\n",
    "    # training\n",
    "    train:bool = True\n",
    "    dataset_path = \"/kaggle/input/clenaned-pretrain-data/cleaned_pre-data_final.txt\" if os.path.exists(\"/kaggle/input/clenaned-pretrain-data/cleaned_pre-data_final.txt\") else \"8k_data.txt\"\n",
    "\n",
    "def precompute_freqs_cis(args: ModelArgs) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "Rotary pozisyonel gömmeler (rotary positional embeddings) için frekansa dayalı kompleks üstel değerleri önceden hesaplar.\n",
    "\n",
    "Parametreler (Args):\n",
    "    args (ModelArgs): Pozisyonel gömme parametrelerini içeren model argümanları.\n",
    "\n",
    "Dönüş (Returns):\n",
    "    torch.Tensor: Pozisyonlara karşılık gelen karmaşık (complex) üstel değerleri içeren bir tensor.\n",
    "    \"\"\"\n",
    "    dim = args.qk_rope_head_dim\n",
    "    seqlen = args.max_seq_len\n",
    "    beta_fast = args.beta_fast # frekans limits\n",
    "    beta_slow = args.beta_slow\n",
    "    base = args.rope_theta\n",
    "    factor = args.rope_factor\n",
    "\n",
    "    #? Belirtilen rotasyon sayısı için dönme açısı 2π·num_rot eşik değerini geçen boyut indeksini hesaplar\n",
    "    def find_correction_dim(num_rotations, dim, base, max_seq_len):\n",
    "\n",
    "        return dim * math.log(max_seq_len / (num_rotations * 2 * math.pi)) / (2 * math.log(base))\n",
    "\n",
    "    #? Dönme açısının bozulmaya başladığı ve tamamen bozulduğu boyut aralığını belirler\n",
    "    def find_correction_range(low_rot, high_rot, dim, base, max_seq_len):\n",
    "\n",
    "        low = math.floor(find_correction_dim(low_rot, dim, base, max_seq_len))\n",
    "        high = math.ceil(find_correction_dim(high_rot, dim, base, max_seq_len))\n",
    "        return max(low, 0), min(high, dim-1)\n",
    "\n",
    "    #? Belirtilen aralıkta [0,1] arasında doğrusal artan bir geçiş (ramp) vektörü oluşturur\n",
    "    def linear_ramp_factor(min, max, dim):\n",
    "\n",
    "        if min == max:\n",
    "            max += 0.001\n",
    "        linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n",
    "        ramp_func = torch.clamp(linear_func, 0, 1)\n",
    "        return ramp_func\n",
    "\n",
    "    freqs = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
    "    \n",
    "    #? Eğer dizi uzunluğu pretraining sınırını aşıyorsa, frekansları yumuşakça düzelt\n",
    "    if seqlen > args.original_seq_len:\n",
    "        low, high = find_correction_range(beta_fast, beta_slow, dim, base, args.original_seq_len)\n",
    "        smooth = 1 - linear_ramp_factor(low, high, dim // 2)\n",
    "        freqs = freqs / factor * (1 - smooth) + freqs * smooth\n",
    "\n",
    "    t = torch.arange(seqlen)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    assert x.shape[-1] % 2 == 0, \"Rotary dim must be divisible by 2!\"\n",
    "    dtype = x.dtype\n",
    "    x = torch.view_as_complex(x.float().view(*x.shape[:-1], -1, 2))\n",
    "    freqs_cis = freqs_cis.view(1, x.size(1), 1, x.size(-1))\n",
    "    y = torch.view_as_real(x * freqs_cis).reshape(*x.shape[:-1], -1)\n",
    "    return y.to(dtype)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, dim:int, eps:float=1e-3):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x:torch.Tensor):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True)+ self.eps)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        return self.weight * self._norm(x.float()).type_as(x)\n",
    "\n",
    "class MLA(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "        Öznitelikler (Attributes):\n",
    "            dim (int): Girdi özelliklerinin boyutu (modelin genel gizli boyutu).\n",
    "            n_heads (int): Dikkat (attention) başlığı sayısı.\n",
    "            n_local_heads (int): Dağıtık sistemler için kullanılan lokal attention başlığı sayısı.\n",
    "            q_lora_rank (int): Query projeksiyonları için düşük-rank (low-rank) LoRA matrislerinin rank değeri.\n",
    "            kv_lora_rank (int): Key/Value projeksiyonları (C^kv) için düşük-rank LoRA rank değeri.\n",
    "            qk_nope_head_dim (int): Konumsal bilgi içermeyen query/key projeksiyonlarının boyutu.\n",
    "            qk_rope_head_dim (int): Rotary positional encoding uygulanan query/key projeksiyonlarının boyutu.\n",
    "            qk_head_dim (int): Query ve key projeksiyonlarının toplam boyutu.\n",
    "            v_head_dim (int): Value (değer) projeksiyonlarının boyutu.\n",
    "            softmax_scale (float): Attention hesaplamalarında softmax’a uygulanan ölçekleme faktörü.\n",
    "    \"\"\"\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        self.dim = args.dim\n",
    "        self.n_head = args.n_heads\n",
    "        self.n_local_head = args.n_heads // 1\n",
    "        self.q_lora_rank = args.q_lora_rank\n",
    "        self.kv_lora_rank = args.kv_lora_rank\n",
    "        self.qk_nope_head_dim = args.qk_nope_head_dim\n",
    "        self.qk_rope_head_dim = args.qk_rope_head_dim\n",
    "        self.v_head_dim = args.v_head_dim\n",
    "        self.qk_head_dim = args.qk_nope_head_dim + args.qk_rope_head_dim\n",
    "        self.isTrain = args.train\n",
    "\n",
    "        if self.q_lora_rank == 0:\n",
    "            self.wq = nn.Linear(self.dim, self.n_head * self.qk_head_dim)\n",
    "        else:\n",
    "            self.wq_a = nn.Linear(self.dim, self.q_lora_rank) # W_DQ\n",
    "            self.q_norm = RMSNorm(self.q_lora_rank)\n",
    "            self.wq_b = nn.Linear(self.q_lora_rank, self.n_head * self.qk_head_dim) # in features: c_t^Q  out features: q_t^C\n",
    "        \n",
    "        self.wkv_a = nn.Linear(self.dim, self.kv_lora_rank + self.qk_rope_head_dim) # burada W^DKV ile W_ht^Kr hesaplamaları birliştirildi\n",
    "        self.kv_norm = RMSNorm(self.kv_lora_rank)\n",
    "        self.wkv_b = nn.Linear(self.kv_lora_rank, self.n_head * (self.qk_nope_head_dim + self.v_head_dim)) # burada W^uk  x c_t^kv işlemi ile W^uv x c_t^kv işlemleri birleştiriliyor\n",
    "        self.wo = nn.Linear(self.n_head * self.v_head_dim, self.dim)\n",
    "        self.softmax_scale = self.qk_head_dim ** -0.5\n",
    "\n",
    "        if args.max_seq_len > args.original_seq_len:\n",
    "            mscale = 0.1 * args.mscale * math.log(args.rope_factor) + 1.0\n",
    "            self.softmax_scale = self.softmax_scale * mscale * mscale\n",
    "\n",
    "\n",
    "        self.register_buffer('kv_cache', torch.zeros(args.max_batch_size, args.max_seq_len, self.kv_lora_rank), persistent=False) # K/V head'lerinin üretildi latent space\n",
    "        self.register_buffer('pe_cache', torch.zeros(args.max_batch_size, args.max_seq_len, self.qk_rope_head_dim), persistent=False) # Pozisyon bilgisini bellekte tutma\n",
    "\n",
    "    def forward(self, x:torch.Tensor, start_pos:int, freqs_cis:torch.Tensor, mask:Optional[torch.Tensor]):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        end_pos = start_pos + seq_len\n",
    "        \n",
    "        if self.q_lora_rank == 0:\n",
    "            q = self.wq(x)\n",
    "        else:\n",
    "            q = self.wq_b(self.q_norm(self.wq_a(x))) # full q_t^c query vector\n",
    "\n",
    "        q = q.view(batch_size,seq_len, self.n_local_head, self.qk_head_dim) # Divide q into heads\n",
    "        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1) # birlikte hesapladığımız q ve q_rope değerlerini ayırıyoruz\n",
    "        q_pe = apply_rotary_emb(q_pe, freqs_cis)\n",
    "        kv = self.wkv_a(x)\n",
    "        kv, k_pe = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim],dim=-1)\n",
    "        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis) # k_pe batch_size içermediğinden ona batch_size boyutu ekliyoruz\n",
    "        \n",
    "\n",
    "        # deepseek tarzı attention hesaplaması\n",
    "        wkv_b = self.wkv_b.weight\n",
    "        wkv_b = wkv_b.view(self.n_local_head, -1, self.kv_lora_rank)\n",
    "        q_nope = torch.einsum('bshd,hdc->bshc', q_nope, wkv_b[:, :self.qk_nope_head_dim])\n",
    "        if not self.isTrain:\n",
    "                    \n",
    "            self.kv_cache[:batch_size, start_pos:end_pos] = self.kv_norm(kv)\n",
    "            self.pe_cache[:batch_size, start_pos:end_pos] = k_pe.squeeze(2)\n",
    "\n",
    "        assert q_nope.shape[-1] == self.kv_cache.shape[-1], \"Head dim mismatch between q_nope and kv_cache\" \n",
    "        kv = self.kv_cache[:batch_size, :end_pos].unsqueeze(2)  # -> [B, T, 1, R]\n",
    "        pe = self.pe_cache[:batch_size, :end_pos].unsqueeze(2)  # -> [B, T, 1, R]\n",
    "        scores = (\n",
    "             torch.einsum('bshr,bthr->bsht', q_nope, kv) +\n",
    "             torch.einsum('bshr,bthr->bsht', q_pe, pe)\n",
    "            ) * self.softmax_scale\n",
    "\n",
    "        if mask is None and end_pos > 1:\n",
    "            mask = torch.full((end_pos, end_pos), float('-inf'), device=x.device).triu(1)\n",
    "\n",
    "        scores = scores.softmax(dim=-1, dtype=torch.float32).type_as(x)\n",
    "\n",
    "        x = torch.einsum('bsht,btc->bshc',scores, self.kv_cache[:batch_size, :end_pos])\n",
    "        x = torch.einsum('bshc,hdc->bshd',x,wkv_b[:,-self.v_head_dim:])\n",
    "        x = self.wo(x.flatten(2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = ModelArgs()\n",
    "mla = MLA(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(4,512,cfg.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 16])\n"
     ]
    }
   ],
   "source": [
    "freqs = precompute_freqs_cis(cfg)\n",
    "print(freqs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 512, 1, 16]' is invalid for input of size 4096",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m b \u001b[38;5;241m=\u001b[39m mla(a,\u001b[38;5;241m0\u001b[39m,freqs,mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 210\u001b[0m, in \u001b[0;36mMLA.forward\u001b[0;34m(self, x, start_pos, freqs_cis, mask)\u001b[0m\n\u001b[1;32m    208\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(batch_size,seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_local_head, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqk_head_dim) \u001b[38;5;66;03m# Divide q into heads\u001b[39;00m\n\u001b[1;32m    209\u001b[0m q_nope, q_pe \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(q, [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqk_nope_head_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqk_rope_head_dim], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# birlikte hesapladığımız q ve q_rope değerlerini ayırıyoruz\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m q_pe \u001b[38;5;241m=\u001b[39m apply_rotary_emb(q_pe, freqs_cis)\n\u001b[1;32m    211\u001b[0m kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwkv_a(x)\n\u001b[1;32m    212\u001b[0m kv, k_pe \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(kv, [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_lora_rank, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqk_rope_head_dim],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 132\u001b[0m, in \u001b[0;36mapply_rotary_emb\u001b[0;34m(x, freqs_cis)\u001b[0m\n\u001b[1;32m    130\u001b[0m dtype \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    131\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_complex(x\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m--> 132\u001b[0m freqs_cis \u001b[38;5;241m=\u001b[39m freqs_cis\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    133\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(x \u001b[38;5;241m*\u001b[39m freqs_cis)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\u001b[38;5;241m.\u001b[39mto(dtype)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 512, 1, 16]' is invalid for input of size 4096"
     ]
    }
   ],
   "source": [
    "b = mla(a,0,freqs,mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
